#!/usr/bin/env python3
from argparse import ArgumentParser
from subprocess import check_call
from pathlib import Path
import os

def main():
    p = ArgumentParser()
    p.add_argument('--json', type=Path, default=Path('menus.jl'))
    p.add_argument('--base-url', type=str, default='https://deliveroo.co.uk')
    p.add_argument('--area', required=True)
    p.add_argument('--postcode', required=True)
    args = p.parse_args()

    # shit. if part of scraping fails, scrapy doesn't necessarily exit with code 1
    # so easiest seems to be checking externally.
    jpath_tmp = args.json.with_suffix('.tmp.jl')
    if jpath_tmp.exists():
        jpath_tmp.unlink()

    cmd = [
        'scrapy', 'crawl', 'deliveroo_spider',
        '-o', str(jpath_tmp),
        '-s', 'LOG_LEVEL=INFO',
        '-s', 'BASE_URL=' + args.base_url,
        '-s', 'POSTCODE=' + args.postcode,
        '-s', 'AREA=' + args.area
    ]
    print("scraping: " + ' '.join(cmd))
    check_call(cmd, cwd=Path(__file__).parent)

    if len(jpath_tmp.read_text().splitlines()) == 0:
        raise RuntimeError("Scrapy didn't receive any results :(")

    jpath_tmp.rename(args.json)


if __name__ == '__main__':
    main()
